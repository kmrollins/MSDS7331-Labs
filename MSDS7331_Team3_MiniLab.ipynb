{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSDS 7331: Data Mining\n",
    "\n",
    "## Mini Lab: Logistic Regression and SVMs\n",
    "\n",
    "## 9 June 2019\n",
    "\n",
    "## Authors: Meredith Ludlow, Anand Rajan, Kristen Rollins, and Tej Tenmattam\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Rubric 1:</b> Create a logistic regression model and a support vector machine model for the classification task involved with your dataset. Assess how well each model performs (use 80/20 training/testing split for your data). Adjust parameters of the models to make them more accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Set seaborn plot styles\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_color_codes('muted')\n",
    "\n",
    "#uncomment when ready to turn in report\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\") # ignore warnings for clean report\n",
    "\n",
    "# df.head() displays all the columns without truncating\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# read csv file as pandas dataframe\n",
    "df_17_census = pd.read_csv('Data/acs2017_census_tract_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean dataset as in lab 1\n",
    "df_17_census.set_index('TractId', inplace=True) # set tract id as index\n",
    "\n",
    "# Drop tracts where population is 0\n",
    "df_17_cln = df_17_census.drop(df_17_census[df_17_census.TotalPop == 0].index)\n",
    "\n",
    "# Drop tracts where child poverty or unemployment is null\n",
    "df_17_cln = df_17_cln[np.isfinite(df_17_cln['ChildPoverty'])]\n",
    "df_17_cln = df_17_cln[np.isfinite(df_17_cln['Unemployment'])]\n",
    "\n",
    "# Impute to the median by each state\n",
    "df_grouped = df_17_cln.groupby('State').transform(lambda x: x.fillna(x.median()))\n",
    "df_17_cln['Income'] = df_grouped['Income']\n",
    "df_17_cln['IncomeErr'] = df_grouped['IncomeErr']\n",
    "\n",
    "# Impute remaining values to the overall median\n",
    "df_17_cln = df_17_cln.fillna(df_17_cln.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 72889 entries, 1001020100 to 72153750602\n",
      "Data columns (total 37 columns):\n",
      "State               72889 non-null object\n",
      "County              72889 non-null object\n",
      "TotalPop            72889 non-null int64\n",
      "Men                 72889 non-null int64\n",
      "Women               72889 non-null int64\n",
      "Hispanic            72889 non-null float64\n",
      "White               72889 non-null float64\n",
      "Black               72889 non-null float64\n",
      "Native              72889 non-null float64\n",
      "Asian               72889 non-null float64\n",
      "Pacific             72889 non-null float64\n",
      "VotingAgeCitizen    72889 non-null int64\n",
      "Income              72889 non-null float64\n",
      "IncomeErr           72889 non-null float64\n",
      "IncomePerCap        72889 non-null float64\n",
      "IncomePerCapErr     72889 non-null float64\n",
      "Poverty             72889 non-null float64\n",
      "ChildPoverty        72889 non-null float64\n",
      "Professional        72889 non-null float64\n",
      "Service             72889 non-null float64\n",
      "Office              72889 non-null float64\n",
      "Construction        72889 non-null float64\n",
      "Production          72889 non-null float64\n",
      "Drive               72889 non-null float64\n",
      "Carpool             72889 non-null float64\n",
      "Transit             72889 non-null float64\n",
      "Walk                72889 non-null float64\n",
      "OtherTransp         72889 non-null float64\n",
      "WorkAtHome          72889 non-null float64\n",
      "MeanCommute         72889 non-null float64\n",
      "Employed            72889 non-null int64\n",
      "PrivateWork         72889 non-null float64\n",
      "PublicWork          72889 non-null float64\n",
      "SelfEmployed        72889 non-null float64\n",
      "FamilyWork          72889 non-null float64\n",
      "Unemployment        72889 non-null float64\n",
      "HighUnemployment    72889 non-null int32\n",
      "dtypes: float64(29), int32(1), int64(5), object(2)\n",
      "memory usage: 20.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Categorize the unemployed percentages into binary categories\n",
    "# Make cutoff using median of clean dataset, so groups are roughly equal\n",
    "df_17_cln['HighUnemployment'] = pd.cut(df_17_cln.Unemployment,[-1,6,101],labels=[0,1])                                 \n",
    "df_17_cln.HighUnemployment = df_17_cln.HighUnemployment.astype(np.int)\n",
    "# 0 indicates low unemployment rate, 1 indicates high unemployment rate\n",
    "\n",
    "df_17_cln.info() # matches cleaned dataset from lab 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 72889 entries, 1001020100 to 72153750602\n",
      "Data columns (total 28 columns):\n",
      "TotalPop            72889 non-null int64\n",
      "Women               72889 non-null float64\n",
      "Hispanic            72889 non-null float64\n",
      "White               72889 non-null float64\n",
      "Black               72889 non-null float64\n",
      "Native              72889 non-null float64\n",
      "Asian               72889 non-null float64\n",
      "Pacific             72889 non-null float64\n",
      "VotingAgeCitizen    72889 non-null float64\n",
      "Income              72889 non-null float64\n",
      "Poverty             72889 non-null float64\n",
      "ChildPoverty        72889 non-null float64\n",
      "Professional        72889 non-null float64\n",
      "Service             72889 non-null float64\n",
      "Office              72889 non-null float64\n",
      "Construction        72889 non-null float64\n",
      "Production          72889 non-null float64\n",
      "Drive               72889 non-null float64\n",
      "Carpool             72889 non-null float64\n",
      "Walk                72889 non-null float64\n",
      "OtherTransp         72889 non-null float64\n",
      "WorkAtHome          72889 non-null float64\n",
      "MeanCommute         72889 non-null float64\n",
      "Employed            72889 non-null float64\n",
      "PrivateWork         72889 non-null float64\n",
      "SelfEmployed        72889 non-null float64\n",
      "FamilyWork          72889 non-null float64\n",
      "HighUnemployment    72889 non-null int32\n",
      "dtypes: float64(26), int32(1), int64(1)\n",
      "memory usage: 15.8 MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Pacific</th>\n",
       "      <th>VotingAgeCitizen</th>\n",
       "      <th>Income</th>\n",
       "      <th>Poverty</th>\n",
       "      <th>ChildPoverty</th>\n",
       "      <th>Professional</th>\n",
       "      <th>Service</th>\n",
       "      <th>Office</th>\n",
       "      <th>Construction</th>\n",
       "      <th>Production</th>\n",
       "      <th>Drive</th>\n",
       "      <th>Carpool</th>\n",
       "      <th>Walk</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>HighUnemployment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TractId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1001020100</th>\n",
       "      <td>1845</td>\n",
       "      <td>51.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>86.3</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.3</td>\n",
       "      <td>67826.0</td>\n",
       "      <td>10.7</td>\n",
       "      <td>20.8</td>\n",
       "      <td>38.5</td>\n",
       "      <td>15.6</td>\n",
       "      <td>22.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>12.4</td>\n",
       "      <td>94.2</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>47.8</td>\n",
       "      <td>74.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001020200</th>\n",
       "      <td>2172</td>\n",
       "      <td>46.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>41.6</td>\n",
       "      <td>54.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.1</td>\n",
       "      <td>41287.0</td>\n",
       "      <td>22.4</td>\n",
       "      <td>35.8</td>\n",
       "      <td>30.5</td>\n",
       "      <td>24.9</td>\n",
       "      <td>22.9</td>\n",
       "      <td>6.3</td>\n",
       "      <td>15.4</td>\n",
       "      <td>90.5</td>\n",
       "      <td>9.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.2</td>\n",
       "      <td>39.2</td>\n",
       "      <td>75.9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001020300</th>\n",
       "      <td>3385</td>\n",
       "      <td>54.7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>61.4</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>73.3</td>\n",
       "      <td>46806.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>21.1</td>\n",
       "      <td>27.9</td>\n",
       "      <td>19.4</td>\n",
       "      <td>33.3</td>\n",
       "      <td>9.9</td>\n",
       "      <td>9.6</td>\n",
       "      <td>88.3</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>23.1</td>\n",
       "      <td>43.8</td>\n",
       "      <td>73.3</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001020400</th>\n",
       "      <td>4267</td>\n",
       "      <td>53.1</td>\n",
       "      <td>9.6</td>\n",
       "      <td>80.3</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.3</td>\n",
       "      <td>55895.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>29.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>25.8</td>\n",
       "      <td>9.1</td>\n",
       "      <td>19.5</td>\n",
       "      <td>82.3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>25.9</td>\n",
       "      <td>43.3</td>\n",
       "      <td>75.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001020500</th>\n",
       "      <td>9965</td>\n",
       "      <td>49.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>77.5</td>\n",
       "      <td>16.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.5</td>\n",
       "      <td>68143.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>17.9</td>\n",
       "      <td>48.8</td>\n",
       "      <td>13.8</td>\n",
       "      <td>20.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>13.4</td>\n",
       "      <td>86.9</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>21.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>71.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            TotalPop  Women  Hispanic  White  Black  Native  Asian  Pacific  \\\n",
       "TractId                                                                       \n",
       "1001020100      1845   51.3       2.4   86.3    5.2     0.0    1.2      0.0   \n",
       "1001020200      2172   46.3       1.1   41.6   54.5     0.0    1.0      0.0   \n",
       "1001020300      3385   54.7       8.0   61.4   26.5     0.6    0.7      0.4   \n",
       "1001020400      4267   53.1       9.6   80.3    7.1     0.5    0.2      0.0   \n",
       "1001020500      9965   49.3       0.9   77.5   16.4     0.0    3.1      0.0   \n",
       "\n",
       "            VotingAgeCitizen   Income  Poverty  ChildPoverty  Professional  \\\n",
       "TractId                                                                      \n",
       "1001020100              76.3  67826.0     10.7          20.8          38.5   \n",
       "1001020200              76.1  41287.0     22.4          35.8          30.5   \n",
       "1001020300              73.3  46806.0     14.7          21.1          27.9   \n",
       "1001020400              76.3  55895.0      2.3           1.7          29.0   \n",
       "1001020500              72.5  68143.0     12.2          17.9          48.8   \n",
       "\n",
       "            Service  Office  Construction  Production  Drive  Carpool  Walk  \\\n",
       "TractId                                                                       \n",
       "1001020100     15.6    22.8          10.8        12.4   94.2      3.3   0.5   \n",
       "1001020200     24.9    22.9           6.3        15.4   90.5      9.1   0.0   \n",
       "1001020300     19.4    33.3           9.9         9.6   88.3      8.4   1.0   \n",
       "1001020400     16.6    25.8           9.1        19.5   82.3     11.2   1.5   \n",
       "1001020500     13.8    20.5           3.5        13.4   86.9     11.2   0.8   \n",
       "\n",
       "            OtherTransp  WorkAtHome  MeanCommute  Employed  PrivateWork  \\\n",
       "TractId                                                                   \n",
       "1001020100          0.0         2.1         24.5      47.8         74.2   \n",
       "1001020200          0.5         0.0         22.2      39.2         75.9   \n",
       "1001020300          0.8         1.5         23.1      43.8         73.3   \n",
       "1001020400          2.9         2.1         25.9      43.3         75.8   \n",
       "1001020500          0.3         0.7         21.0      48.0         71.4   \n",
       "\n",
       "            SelfEmployed  FamilyWork  HighUnemployment  \n",
       "TractId                                                 \n",
       "1001020100           4.5         0.0                 0  \n",
       "1001020200           9.0         0.0                 0  \n",
       "1001020300           4.8         0.7                 0  \n",
       "1001020400           4.5         0.0                 1  \n",
       "1001020500           4.5         0.0                 0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe to use in logistic and SVM models\n",
    "df_17_model = df_17_cln.copy()\n",
    "\n",
    "# Eliminate non-useful and redundant variables\n",
    "del df_17_model['State'] # encoding would result in too many added variables\n",
    "del df_17_model['County'] # encoding would result in too many added variables\n",
    "del df_17_model['Men'] # redundant to keep men and women\n",
    "del df_17_model['Unemployment'] # already encoded to binary\n",
    "\n",
    "# Eliminate more variables based on correlation plot (TODO maybe remove more)\n",
    "del df_17_model['IncomeErr'] # only need to keep one income-related variable\n",
    "del df_17_model['IncomePerCap'] # only need to keep one income-related variable\n",
    "del df_17_model['IncomePerCapErr'] # only need to keep one income-related variable\n",
    "del df_17_model['Transit'] # drive and transit were essentially inverses\n",
    "del df_17_model['PublicWork'] # private and public work were essentially inverses\n",
    "\n",
    "# All remaining variables are ints or floats so we do not have to do one-hot encoding\n",
    "\n",
    "# Convert columns to percentages for consistency\n",
    "df_17_model['Women'] = round(df_17_model['Women']/df_17_model['TotalPop']*100,1)\n",
    "df_17_model['VotingAgeCitizen'] = round(df_17_model['VotingAgeCitizen']/df_17_model['TotalPop']*100,1)\n",
    "df_17_model['Employed'] = round(df_17_model['Employed']/df_17_model['TotalPop']*100,1)\n",
    "\n",
    "# TODO maybe further dimension reduction via PCA\n",
    "\n",
    "print(df_17_model.info())\n",
    "df_17_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=10, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "# Following code from Dr. Larson's Logits and SVM notebook\n",
    "# Question: should we actually use cross-validation for this lab? (rubric doesn't mention it)\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'HighUnemployment' in df_17_model:\n",
    "    y = df_17_model['HighUnemployment'].values # get the labels we want\n",
    "    del df_17_model['HighUnemployment'] # get rid of the class label\n",
    "    X = df_17_model.values # use everything else to predict\n",
    "    \n",
    "# 10 fold cross-validation\n",
    "num_cv_iterations = 10\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "Precision: 0.7574173712528824\n",
      "Recall: 0.6988652482269504\n",
      "accuracy 0.7461242968857182\n",
      "confusion matrix\n",
      " [[5950 1578]\n",
      " [2123 4927]]\n",
      "====Iteration 1  ====\n",
      "Precision: 0.7676204819277108\n",
      "Recall: 0.708408617095205\n",
      "accuracy 0.7502400878035396\n",
      "confusion matrix\n",
      " [[5840 1543]\n",
      " [2098 5097]]\n",
      "====Iteration 2  ====\n",
      "Precision: 0.7630741246020919\n",
      "Recall: 0.7012118679481822\n",
      "accuracy 0.745644121278639\n",
      "confusion matrix\n",
      " [[5836 1563]\n",
      " [2145 5034]]\n",
      "====Iteration 3  ====\n",
      "Precision: 0.7644546147978643\n",
      "Recall: 0.6977165135059872\n",
      "accuracy 0.7451639456715599\n",
      "confusion matrix\n",
      " [[5852 1544]\n",
      " [2171 5011]]\n",
      "====Iteration 4  ====\n",
      "Precision: 0.7664943751900274\n",
      "Recall: 0.7032078103207811\n",
      "accuracy 0.748662367951708\n",
      "confusion matrix\n",
      " [[5872 1536]\n",
      " [2128 5042]]\n",
      "====Iteration 5  ====\n",
      "Precision: 0.7534534534534535\n",
      "Recall: 0.6964607911172797\n",
      "accuracy 0.7373439429276992\n",
      "confusion matrix\n",
      " [[5731 1642]\n",
      " [2187 5018]]\n",
      "====Iteration 6  ====\n",
      "Precision: 0.7557519427091269\n",
      "Recall: 0.6939974814607528\n",
      "accuracy 0.7400192070242831\n",
      "confusion matrix\n",
      " [[5828 1603]\n",
      " [2187 4960]]\n",
      "====Iteration 7  ====\n",
      "Precision: 0.7664943751900274\n",
      "Recall: 0.6997918112421929\n",
      "accuracy 0.7462614899163122\n",
      "confusion matrix\n",
      " [[5837 1536]\n",
      " [2163 5042]]\n",
      "====Iteration 8  ====\n",
      "Precision: 0.7619849900444172\n",
      "Recall: 0.6966811370956448\n",
      "accuracy 0.7448209630950747\n",
      "confusion matrix\n",
      " [[5883 1554]\n",
      " [2166 4975]]\n",
      "====Iteration 9  ====\n",
      "Precision: 0.7606423269201636\n",
      "Recall: 0.6998884862001673\n",
      "accuracy 0.7439292083962135\n",
      "confusion matrix\n",
      " [[5824 1580]\n",
      " [2153 5021]]\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "# also scale variables\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scl_obj = StandardScaler()\n",
    "\n",
    "# TODO repeat and try out different learning parameters and constants\n",
    "# TODO try to fix warnings - fixed this with solver\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05, class_weight=None, solver='lbfgs') \n",
    "        #class_weight=None because unemployment groups are weighted equally\n",
    "\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    # scale attributes by the training set\n",
    "    scl_obj.fit(X[train_indices])\n",
    "    X_train_scaled = scl_obj.transform(X[train_indices]) # apply to training\n",
    "    X_test_scaled = scl_obj.transform(X[test_indices]) # use training scales to adjust test set, so we're not cheating\n",
    "    \n",
    "    # train object\n",
    "    lr_clf.fit(X_train_scaled,y[train_indices])\n",
    "    # get test set precitions\n",
    "    y_hat = lr_clf.predict(X_test_scaled)\n",
    "\n",
    "    # print the accuracy and confusion matrix \n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"Precision:\",mt.precision_score(y[test_indices],y_hat))\n",
    "    print(\"Recall:\",mt.recall_score(y[test_indices],y_hat))\n",
    "    print(\"accuracy\", mt.accuracy_score(y[test_indices],y_hat)) \n",
    "    print(\"confusion matrix\\n\",mt.confusion_matrix(y[test_indices],y_hat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ROC Curve for LR model above\n",
    "y_pred_prob = lr_clf.predict_proba(X_test_scaled)[::,1]\n",
    "fpr, tpr, _ = mt.roc_curve(y[test_indices],  y_pred_prob)\n",
    "auc = mt.roc_auc_score(y[test_indices], y_pred_prob)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54010229, 0.47975858, 0.35000395, ..., 0.66487682, 0.50159344,\n",
       "        0.40692225],\n",
       "       [0.4062564 , 0.67007188, 0.41935135, ..., 0.50247258, 0.34383277,\n",
       "        0.40692225],\n",
       "       [0.40471001, 0.34722001, 0.87636864, ..., 0.47747995, 0.48202218,\n",
       "        0.40692225],\n",
       "       ...,\n",
       "       [0.65060586, 0.80255351, 0.3529563 , ..., 0.23541475, 0.43667197,\n",
       "        0.40692225],\n",
       "       [0.32630483, 0.47379035, 0.39538364, ..., 0.72565623, 0.43667197,\n",
       "        0.40692225],\n",
       "       [0.65806742, 0.20214333, 0.36788333, ..., 0.20948507, 0.4755077 ,\n",
       "        0.72189229]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import numpy as np\n",
    "import argparse\n",
    " \n",
    "def sigmoid_activation(x):\n",
    "# compute and return the sigmoid activation value for a\n",
    "# given input value\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "sigmoid_activation(X_train_scaled)\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anand\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"\n",
      "C:\\Users\\Anand\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in multiply\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss function for gradient descent y=0 to y=1 and y_hat is prediction for best value\n",
    "# weights and bias are updated until we reach bottom (the minimum point)\n",
    "\n",
    "def log_loss (y, y_hat):\n",
    "    return -np.mean(y * np.log(y_hat) + (1 -y) * np.log(1-y_hat))\n",
    "\n",
    "log_loss(y[test_indices],y_hat)\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anand\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"\n",
      "C:\\Users\\Anand\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in multiply\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and plot the result\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "plt.scatter(X.ravel(), y, color='black', zorder=20)\n",
    "X_test = np.linspace(-5, 10, 300)\n",
    "\n",
    "loss = expit(X_test * clf.coef_ + clf.intercept_).ravel()\n",
    "plt.plot(X_test, loss, color='red', linewidth=3)\n",
    "\n",
    "ols = linear_model.LinearRegression()\n",
    "ols.fit(X, y)\n",
    "plt.plot(X_test, ols.coef_ * X_test + ols.intercept_, linewidth=1)\n",
    "plt.axhline(.5, color='.5')\n",
    "\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('X')\n",
    "plt.xticks(range(-5, 10))\n",
    "plt.yticks([0, 0.5, 1])\n",
    "plt.ylim(-.25, 1.25)\n",
    "plt.xlim(-4, 10)\n",
    "plt.legend(('Logistic Regression Model', 'Linear Regression Model'),\n",
    "           loc=\"lower right\", fontsize='small')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, so run through the cross validation loop and set the training and testing variable for one single iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.748525174921114\n",
      "[[5773 1680]\n",
      " [1986 5139]]\n"
     ]
    }
   ],
   "source": [
    "# lets investigate SVMs on the data and play with the parameters and kernels\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "svm_clf = SVC(C=0.5, kernel='rbf', degree=3, gamma='auto') # get object\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33139, 27)\n",
      "(33139,)\n",
      "[16406 16733]\n"
     ]
    }
   ],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Rubric 2:</b> Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Rubric 3:</b> Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White has weight of -1.0877580661437716\n",
      "Employed has weight of -0.6433136267451702\n",
      "Hispanic has weight of -0.46595382599076873\n",
      "Black has weight of -0.305921103731918\n",
      "Asian has weight of -0.21864260396207214\n",
      "Income has weight of -0.15275117978917715\n",
      "Professional has weight of -0.11232045473800638\n",
      "ChildPoverty has weight of -0.08130040158344763\n",
      "WorkAtHome has weight of -0.07040789803161154\n",
      "Pacific has weight of -0.06412586057660227\n",
      "Walk has weight of -0.05346050022263001\n",
      "TotalPop has weight of -0.039475477851311884\n",
      "Construction has weight of -0.03939367826950078\n",
      "Native has weight of -0.03458302017462102\n",
      "FamilyWork has weight of -0.030590715189962498\n",
      "SelfEmployed has weight of -0.02219771023257836\n",
      "Drive has weight of -0.00445408617184769\n",
      "OtherTransp has weight of 0.03818328156548312\n",
      "PrivateWork has weight of 0.044271905533371896\n",
      "Office has weight of 0.04518728654711343\n",
      "Production has weight of 0.057514215876318386\n",
      "Carpool has weight of 0.05927099932857496\n",
      "Women has weight of 0.0793037441503722\n",
      "Service has weight of 0.17570047532786548\n",
      "MeanCommute has weight of 0.27276047902333417\n",
      "VotingAgeCitizen has weight of 0.2865633569513734\n",
      "Poverty has weight of 0.6131510437049636\n"
     ]
    }
   ],
   "source": [
    "# TODO use best logistic model we find (lr_clf object)\n",
    "# sort attributes by weight and spit them out\n",
    "zip_vars = zip(lr_clf.coef_.T,df_17_model.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret Support Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Rubric 4:</b> Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC model then analyze the support vectors from the subsampled dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
