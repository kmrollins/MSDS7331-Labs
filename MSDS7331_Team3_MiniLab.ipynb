{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSDS 7331: Data Mining\n",
    "\n",
    "## Mini Lab: Logistic Regression and SVMs\n",
    "\n",
    "## 9 June 2019\n",
    "\n",
    "## Authors: Meredith Ludlow, Anand Rajan, Kristen Rollins, and Tej Tenmattam\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Rubric 1:</b> Create a logistic regression model and a support vector machine model for the classification task involved with your dataset. Assess how well each model performs (use 80/20 training/testing split for your data). Adjust parameters of the models to make them more accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Set seaborn plot styles\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_color_codes('muted')\n",
    "\n",
    "#uncomment when ready to turn in report\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\") # ignore warnings for clean report\n",
    "\n",
    "# df.head() displays all the columns without truncating\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# read csv file as pandas dataframe\n",
    "df_17_census = pd.read_csv('Data/acs2017_census_tract_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean dataset as in lab 1\n",
    "df_17_census.set_index('TractId', inplace=True) # set tract id as index\n",
    "\n",
    "# Drop tracts where population is 0\n",
    "df_17_cln = df_17_census.drop(df_17_census[df_17_census.TotalPop == 0].index)\n",
    "\n",
    "# Drop tracts where child poverty or unemployment is null\n",
    "df_17_cln = df_17_cln[np.isfinite(df_17_cln['ChildPoverty'])]\n",
    "df_17_cln = df_17_cln[np.isfinite(df_17_cln['Unemployment'])]\n",
    "\n",
    "# Impute to the median by each state\n",
    "df_grouped = df_17_cln.groupby('State').transform(lambda x: x.fillna(x.median()))\n",
    "df_17_cln['Income'] = df_grouped['Income']\n",
    "df_17_cln['IncomeErr'] = df_grouped['IncomeErr']\n",
    "\n",
    "# Impute remaining values to the overall median\n",
    "df_17_cln = df_17_cln.fillna(df_17_cln.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 72889 entries, 1001020100 to 72153750602\n",
      "Data columns (total 37 columns):\n",
      "State               72889 non-null object\n",
      "County              72889 non-null object\n",
      "TotalPop            72889 non-null int64\n",
      "Men                 72889 non-null int64\n",
      "Women               72889 non-null int64\n",
      "Hispanic            72889 non-null float64\n",
      "White               72889 non-null float64\n",
      "Black               72889 non-null float64\n",
      "Native              72889 non-null float64\n",
      "Asian               72889 non-null float64\n",
      "Pacific             72889 non-null float64\n",
      "VotingAgeCitizen    72889 non-null int64\n",
      "Income              72889 non-null float64\n",
      "IncomeErr           72889 non-null float64\n",
      "IncomePerCap        72889 non-null float64\n",
      "IncomePerCapErr     72889 non-null float64\n",
      "Poverty             72889 non-null float64\n",
      "ChildPoverty        72889 non-null float64\n",
      "Professional        72889 non-null float64\n",
      "Service             72889 non-null float64\n",
      "Office              72889 non-null float64\n",
      "Construction        72889 non-null float64\n",
      "Production          72889 non-null float64\n",
      "Drive               72889 non-null float64\n",
      "Carpool             72889 non-null float64\n",
      "Transit             72889 non-null float64\n",
      "Walk                72889 non-null float64\n",
      "OtherTransp         72889 non-null float64\n",
      "WorkAtHome          72889 non-null float64\n",
      "MeanCommute         72889 non-null float64\n",
      "Employed            72889 non-null int64\n",
      "PrivateWork         72889 non-null float64\n",
      "PublicWork          72889 non-null float64\n",
      "SelfEmployed        72889 non-null float64\n",
      "FamilyWork          72889 non-null float64\n",
      "Unemployment        72889 non-null float64\n",
      "HighUnemployment    72889 non-null int64\n",
      "dtypes: float64(29), int64(6), object(2)\n",
      "memory usage: 21.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Categorize the unemployed percentages into binary categories\n",
    "# Make cutoff using median of clean dataset, so groups are roughly equal\n",
    "df_17_cln['HighUnemployment'] = pd.cut(df_17_cln.Unemployment,[-1,6,101],labels=[0,1])                                 \n",
    "df_17_cln.HighUnemployment = df_17_cln.HighUnemployment.astype(np.int)\n",
    "# 0 indicates low unemployment rate, 1 indicates high unemployment rate\n",
    "\n",
    "df_17_cln.info() # matches cleaned dataset from lab 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 72889 entries, 1001020100 to 72153750602\n",
      "Data columns (total 28 columns):\n",
      "TotalPop            72889 non-null int64\n",
      "Women               72889 non-null float64\n",
      "Hispanic            72889 non-null float64\n",
      "White               72889 non-null float64\n",
      "Black               72889 non-null float64\n",
      "Native              72889 non-null float64\n",
      "Asian               72889 non-null float64\n",
      "Pacific             72889 non-null float64\n",
      "VotingAgeCitizen    72889 non-null float64\n",
      "Income              72889 non-null float64\n",
      "Poverty             72889 non-null float64\n",
      "ChildPoverty        72889 non-null float64\n",
      "Professional        72889 non-null float64\n",
      "Service             72889 non-null float64\n",
      "Office              72889 non-null float64\n",
      "Construction        72889 non-null float64\n",
      "Production          72889 non-null float64\n",
      "Drive               72889 non-null float64\n",
      "Carpool             72889 non-null float64\n",
      "Walk                72889 non-null float64\n",
      "OtherTransp         72889 non-null float64\n",
      "WorkAtHome          72889 non-null float64\n",
      "MeanCommute         72889 non-null float64\n",
      "Employed            72889 non-null float64\n",
      "PrivateWork         72889 non-null float64\n",
      "SelfEmployed        72889 non-null float64\n",
      "FamilyWork          72889 non-null float64\n",
      "HighUnemployment    72889 non-null int64\n",
      "dtypes: float64(26), int64(2)\n",
      "memory usage: 16.1 MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Pacific</th>\n",
       "      <th>VotingAgeCitizen</th>\n",
       "      <th>Income</th>\n",
       "      <th>Poverty</th>\n",
       "      <th>ChildPoverty</th>\n",
       "      <th>Professional</th>\n",
       "      <th>Service</th>\n",
       "      <th>Office</th>\n",
       "      <th>Construction</th>\n",
       "      <th>Production</th>\n",
       "      <th>Drive</th>\n",
       "      <th>Carpool</th>\n",
       "      <th>Walk</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>HighUnemployment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TractId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1001020100</th>\n",
       "      <td>1845</td>\n",
       "      <td>51.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>86.3</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.3</td>\n",
       "      <td>67826.0</td>\n",
       "      <td>10.7</td>\n",
       "      <td>20.8</td>\n",
       "      <td>38.5</td>\n",
       "      <td>15.6</td>\n",
       "      <td>22.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>12.4</td>\n",
       "      <td>94.2</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>47.8</td>\n",
       "      <td>74.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001020200</th>\n",
       "      <td>2172</td>\n",
       "      <td>46.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>41.6</td>\n",
       "      <td>54.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.1</td>\n",
       "      <td>41287.0</td>\n",
       "      <td>22.4</td>\n",
       "      <td>35.8</td>\n",
       "      <td>30.5</td>\n",
       "      <td>24.9</td>\n",
       "      <td>22.9</td>\n",
       "      <td>6.3</td>\n",
       "      <td>15.4</td>\n",
       "      <td>90.5</td>\n",
       "      <td>9.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.2</td>\n",
       "      <td>39.2</td>\n",
       "      <td>75.9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001020300</th>\n",
       "      <td>3385</td>\n",
       "      <td>54.7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>61.4</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>73.3</td>\n",
       "      <td>46806.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>21.1</td>\n",
       "      <td>27.9</td>\n",
       "      <td>19.4</td>\n",
       "      <td>33.3</td>\n",
       "      <td>9.9</td>\n",
       "      <td>9.6</td>\n",
       "      <td>88.3</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>23.1</td>\n",
       "      <td>43.8</td>\n",
       "      <td>73.3</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001020400</th>\n",
       "      <td>4267</td>\n",
       "      <td>53.1</td>\n",
       "      <td>9.6</td>\n",
       "      <td>80.3</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.3</td>\n",
       "      <td>55895.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>29.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>25.8</td>\n",
       "      <td>9.1</td>\n",
       "      <td>19.5</td>\n",
       "      <td>82.3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>25.9</td>\n",
       "      <td>43.3</td>\n",
       "      <td>75.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001020500</th>\n",
       "      <td>9965</td>\n",
       "      <td>49.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>77.5</td>\n",
       "      <td>16.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.5</td>\n",
       "      <td>68143.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>17.9</td>\n",
       "      <td>48.8</td>\n",
       "      <td>13.8</td>\n",
       "      <td>20.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>13.4</td>\n",
       "      <td>86.9</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>21.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>71.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            TotalPop  Women  Hispanic  White  Black  Native  Asian  Pacific  \\\n",
       "TractId                                                                       \n",
       "1001020100      1845   51.3       2.4   86.3    5.2     0.0    1.2      0.0   \n",
       "1001020200      2172   46.3       1.1   41.6   54.5     0.0    1.0      0.0   \n",
       "1001020300      3385   54.7       8.0   61.4   26.5     0.6    0.7      0.4   \n",
       "1001020400      4267   53.1       9.6   80.3    7.1     0.5    0.2      0.0   \n",
       "1001020500      9965   49.3       0.9   77.5   16.4     0.0    3.1      0.0   \n",
       "\n",
       "            VotingAgeCitizen   Income  Poverty  ChildPoverty  Professional  \\\n",
       "TractId                                                                      \n",
       "1001020100              76.3  67826.0     10.7          20.8          38.5   \n",
       "1001020200              76.1  41287.0     22.4          35.8          30.5   \n",
       "1001020300              73.3  46806.0     14.7          21.1          27.9   \n",
       "1001020400              76.3  55895.0      2.3           1.7          29.0   \n",
       "1001020500              72.5  68143.0     12.2          17.9          48.8   \n",
       "\n",
       "            Service  Office  Construction  Production  Drive  Carpool  Walk  \\\n",
       "TractId                                                                       \n",
       "1001020100     15.6    22.8          10.8        12.4   94.2      3.3   0.5   \n",
       "1001020200     24.9    22.9           6.3        15.4   90.5      9.1   0.0   \n",
       "1001020300     19.4    33.3           9.9         9.6   88.3      8.4   1.0   \n",
       "1001020400     16.6    25.8           9.1        19.5   82.3     11.2   1.5   \n",
       "1001020500     13.8    20.5           3.5        13.4   86.9     11.2   0.8   \n",
       "\n",
       "            OtherTransp  WorkAtHome  MeanCommute  Employed  PrivateWork  \\\n",
       "TractId                                                                   \n",
       "1001020100          0.0         2.1         24.5      47.8         74.2   \n",
       "1001020200          0.5         0.0         22.2      39.2         75.9   \n",
       "1001020300          0.8         1.5         23.1      43.8         73.3   \n",
       "1001020400          2.9         2.1         25.9      43.3         75.8   \n",
       "1001020500          0.3         0.7         21.0      48.0         71.4   \n",
       "\n",
       "            SelfEmployed  FamilyWork  HighUnemployment  \n",
       "TractId                                                 \n",
       "1001020100           4.5         0.0                 0  \n",
       "1001020200           9.0         0.0                 0  \n",
       "1001020300           4.8         0.7                 0  \n",
       "1001020400           4.5         0.0                 1  \n",
       "1001020500           4.5         0.0                 0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe to use in logistic and SVM models\n",
    "df_17_model = df_17_cln.copy()\n",
    "\n",
    "# Eliminate non-useful and redundant variables\n",
    "del df_17_model['State'] # encoding would result in too many added variables\n",
    "del df_17_model['County'] # encoding would result in too many added variables\n",
    "del df_17_model['Men'] # redundant to keep men and women\n",
    "del df_17_model['Unemployment'] # already encoded to binary\n",
    "\n",
    "# Eliminate more variables based on correlation plot (TODO maybe remove more)\n",
    "del df_17_model['IncomeErr'] # only need to keep one income-related variable\n",
    "del df_17_model['IncomePerCap'] # only need to keep one income-related variable\n",
    "del df_17_model['IncomePerCapErr'] # only need to keep one income-related variable\n",
    "del df_17_model['Transit'] # drive and transit were essentially inverses\n",
    "del df_17_model['PublicWork'] # private and public work were essentially inverses\n",
    "\n",
    "# All remaining variables are ints or floats so we do not have to do one-hot encoding\n",
    "\n",
    "# Convert columns to percentages for consistency\n",
    "df_17_model['Women'] = round(df_17_model['Women']/df_17_model['TotalPop']*100,1)\n",
    "df_17_model['VotingAgeCitizen'] = round(df_17_model['VotingAgeCitizen']/df_17_model['TotalPop']*100,1)\n",
    "df_17_model['Employed'] = round(df_17_model['Employed']/df_17_model['TotalPop']*100,1)\n",
    "\n",
    "# TODO maybe further dimension reduction via PCA\n",
    "\n",
    "print(df_17_model.info())\n",
    "df_17_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=10, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "# Following code from Dr. Larson's Logits and SVM notebook\n",
    "# Question: should we actually use cross-validation for this lab? (rubric doesn't mention it)\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'HighUnemployment' in df_17_model:\n",
    "    y = df_17_model['HighUnemployment'].values # get the labels we want\n",
    "    del df_17_model['HighUnemployment'] # get rid of the class label\n",
    "    X = df_17_model.values # use everything else to predict\n",
    "    \n",
    "# 10 fold cross-validation\n",
    "num_cv_iterations = 10\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristenmccrary/anaconda3/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.745575524763342\n",
      "confusion matrix\n",
      " [[5789 1536]\n",
      " [2173 5080]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristenmccrary/anaconda3/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 1  ====\n",
      "accuracy 0.7436548223350253\n",
      "confusion matrix\n",
      " [[5762 1563]\n",
      " [2174 5079]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristenmccrary/anaconda3/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 2  ====\n",
      "accuracy 0.7446837700644807\n",
      "confusion matrix\n",
      " [[5803 1555]\n",
      " [2167 5053]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristenmccrary/anaconda3/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 3  ====\n",
      "accuracy 0.7468788585539855\n",
      "confusion matrix\n",
      " [[5903 1546]\n",
      " [2144 4985]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristenmccrary/anaconda3/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 4  ====\n",
      "accuracy 0.7418713129373028\n",
      "confusion matrix\n",
      " [[5731 1637]\n",
      " [2126 5084]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristenmccrary/anaconda3/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 5  ====\n",
      "accuracy 0.745575524763342\n",
      "confusion matrix\n",
      " [[5825 1564]\n",
      " [2145 5044]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristenmccrary/anaconda3/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 6  ====\n",
      "accuracy 0.7457813143092331\n",
      "confusion matrix\n",
      " [[5814 1563]\n",
      " [2143 5058]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristenmccrary/anaconda3/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 7  ====\n",
      "accuracy 0.7463300864316093\n",
      "confusion matrix\n",
      " [[5839 1557]\n",
      " [2141 5041]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristenmccrary/anaconda3/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 8  ====\n",
      "accuracy 0.739676224447798\n",
      "confusion matrix\n",
      " [[5803 1638]\n",
      " [2157 4980]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristenmccrary/anaconda3/envs/data_mining/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 9  ====\n",
      "accuracy 0.7469474550692825\n",
      "confusion matrix\n",
      " [[5892 1535]\n",
      " [2154 4997]]\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "# also scale variables\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scl_obj = StandardScaler()\n",
    "\n",
    "# TODO repeat and try out different learning parameters and constants\n",
    "# TODO try to fix warnings\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05, class_weight=None) \n",
    "        #class_weight=None because unemployment groups are weighted equally\n",
    "\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    # scale attributes by the training set\n",
    "    scl_obj.fit(X[train_indices])\n",
    "    X_train_scaled = scl_obj.transform(X[train_indices]) # apply to training\n",
    "    X_test_scaled = scl_obj.transform(X[test_indices]) # use training scales to adjust test set, so we're not cheating\n",
    "    \n",
    "    # train object\n",
    "    lr_clf.fit(X_train_scaled,y[train_indices])\n",
    "    # get test set precitions\n",
    "    y_hat = lr_clf.predict(X_test_scaled)\n",
    "\n",
    "    # print the accuracy and confusion matrix \n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", mt.accuracy_score(y[test_indices],y_hat)) \n",
    "    print(\"confusion matrix\\n\",mt.confusion_matrix(y[test_indices],y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Rubric 2:</b> Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Rubric 3:</b> Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White has weight of -1.0877580661437716\n",
      "Employed has weight of -0.6433136267451702\n",
      "Hispanic has weight of -0.46595382599076873\n",
      "Black has weight of -0.305921103731918\n",
      "Asian has weight of -0.21864260396207214\n",
      "Income has weight of -0.15275117978917715\n",
      "Professional has weight of -0.11232045473800638\n",
      "ChildPoverty has weight of -0.08130040158344763\n",
      "WorkAtHome has weight of -0.07040789803161154\n",
      "Pacific has weight of -0.06412586057660227\n",
      "Walk has weight of -0.05346050022263001\n",
      "TotalPop has weight of -0.039475477851311884\n",
      "Construction has weight of -0.03939367826950078\n",
      "Native has weight of -0.03458302017462102\n",
      "FamilyWork has weight of -0.030590715189962498\n",
      "SelfEmployed has weight of -0.02219771023257836\n",
      "Drive has weight of -0.00445408617184769\n",
      "OtherTransp has weight of 0.03818328156548312\n",
      "PrivateWork has weight of 0.044271905533371896\n",
      "Office has weight of 0.04518728654711343\n",
      "Production has weight of 0.057514215876318386\n",
      "Carpool has weight of 0.05927099932857496\n",
      "Women has weight of 0.0793037441503722\n",
      "Service has weight of 0.17570047532786548\n",
      "MeanCommute has weight of 0.27276047902333417\n",
      "VotingAgeCitizen has weight of 0.2865633569513734\n",
      "Poverty has weight of 0.6131510437049636\n"
     ]
    }
   ],
   "source": [
    "# TODO use best logistic model we find (lr_clf object)\n",
    "# sort attributes by weight and spit them out\n",
    "zip_vars = zip(lr_clf.coef_.T,df_17_model.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret Support Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Rubric 4:</b> Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC modelâ€” then analyze the support vectors from the subsampled dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
