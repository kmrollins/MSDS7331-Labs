{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSDS 7331: Data Mining\n",
    "\n",
    "## Mini Lab: Logistic Regression and SVMs\n",
    "\n",
    "## 9 June 2019\n",
    "\n",
    "## Authors: Meredith Ludlow, Anand Rajan, Kristen Rollins, and Tej Tenmattam\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Rubric 1:</b> Create a logistic regression model and a support vector machine model for the classification task involved with your dataset. Assess how well each model performs (use 80/20 training/testing split for your data). Adjust parameters of the models to make them more accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Set seaborn plot styles\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_color_codes('muted')\n",
    "\n",
    "#uncomment when ready to turn in report\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\") # ignore warnings for clean report\n",
    "\n",
    "# df.head() displays all the columns without truncating\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# read csv file as pandas dataframe\n",
    "df_17_census = pd.read_csv('Data/acs2017_census_tract_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean dataset as in lab 1\n",
    "df_17_census.set_index('TractId', inplace=True) # set tract id as index\n",
    "\n",
    "#Drop tracts where population is 0\n",
    "df_17_cln = df_17_census.drop(df_17_census[df_17_census.TotalPop == 0].index)\n",
    "\n",
    "#Drop tracts where child poverty or unemployment is null\n",
    "df_17_cln = df_17_cln[np.isfinite(df_17_cln['ChildPoverty'])]\n",
    "df_17_cln = df_17_cln[np.isfinite(df_17_cln['Unemployment'])]\n",
    "\n",
    "#Impute to the median by each state\n",
    "df_grouped = df_17_cln.groupby('State').transform(lambda x: x.fillna(x.median()))\n",
    "df_17_cln['Income'] = df_grouped['Income']\n",
    "df_17_cln['IncomeErr'] = df_grouped['IncomeErr']\n",
    "\n",
    "#Impute remaining values to the overall median\n",
    "df_17_cln = df_17_cln.fillna(df_17_cln.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 72889 entries, 1001020100 to 72153750602\n",
      "Data columns (total 38 columns):\n",
      "State                  72889 non-null object\n",
      "County                 72889 non-null object\n",
      "TotalPop               72889 non-null int64\n",
      "Men                    72889 non-null int64\n",
      "Women                  72889 non-null int64\n",
      "Hispanic               72889 non-null float64\n",
      "White                  72889 non-null float64\n",
      "Black                  72889 non-null float64\n",
      "Native                 72889 non-null float64\n",
      "Asian                  72889 non-null float64\n",
      "Pacific                72889 non-null float64\n",
      "VotingAgeCitizen       72889 non-null int64\n",
      "Income                 72889 non-null float64\n",
      "IncomeErr              72889 non-null float64\n",
      "IncomePerCap           72889 non-null float64\n",
      "IncomePerCapErr        72889 non-null float64\n",
      "Poverty                72889 non-null float64\n",
      "ChildPoverty           72889 non-null float64\n",
      "Professional           72889 non-null float64\n",
      "Service                72889 non-null float64\n",
      "Office                 72889 non-null float64\n",
      "Construction           72889 non-null float64\n",
      "Production             72889 non-null float64\n",
      "Drive                  72889 non-null float64\n",
      "Carpool                72889 non-null float64\n",
      "Transit                72889 non-null float64\n",
      "Walk                   72889 non-null float64\n",
      "OtherTransp            72889 non-null float64\n",
      "WorkAtHome             72889 non-null float64\n",
      "MeanCommute            72889 non-null float64\n",
      "Employed               72889 non-null int64\n",
      "PrivateWork            72889 non-null float64\n",
      "PublicWork             72889 non-null float64\n",
      "SelfEmployed           72889 non-null float64\n",
      "FamilyWork             72889 non-null float64\n",
      "Unemployment           72889 non-null float64\n",
      "Unemployment_Cat       72889 non-null category\n",
      "Unemployment_Binary    72889 non-null category\n",
      "dtypes: category(2), float64(29), int64(5), object(2)\n",
      "memory usage: 20.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Categorize the unemployed percentages\n",
    "# Make cutoffs using quartiles of clean dataset, so they are roughly equal\n",
    "df_17_cln['Unemployment_Cat'] = pd.cut(df_17_cln.Unemployment,[-1,3.9,6,9,101],labels=['low','med','high','very_high'])\n",
    "\n",
    "# Also cut unemployment into binary categories if it's easier to work with\n",
    "df_17_cln['Unemployment_Binary'] = pd.cut(df_17_cln.Unemployment,[-1,6,101],labels=['low','high'])\n",
    "\n",
    "df_17_cln.info() # matches cleaned dataset from lab 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Rubric 2:</b> Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Rubric 3:</b> Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret Support Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Rubric 4:</b> Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC modelâ€” then analyze the support vectors from the subsampled dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
